diff --git a/core/src/main/java/org/apache/hama/bsp/BSPMaster.java b/core/src/main/java/org/apache/hama/bsp/BSPMaster.java
index d8d2540..b4dadde 100644
--- a/core/src/main/java/org/apache/hama/bsp/BSPMaster.java
+++ b/core/src/main/java/org/apache/hama/bsp/BSPMaster.java
@@ -163,20 +163,22 @@ public class BSPMaster implements JobSubmissionProtocol, MasterProtocol,
       // for each dead groomserver in blacklist, mark all failed tasks as recovery state
       while (blackList.size() > 0) {
         GroomServerStatus deadGroomServer = blackList.remove(0);
+        System.out.println("Groom server " + deadGroomServer.getGroomName() + " is dead!");
         List<TaskStatus> deadTasks = deadGroomServer.getTaskReports();
         for (TaskStatus ts : deadTasks) {
             JobInProgress jip = taskScheduler.findJobById(ts.getJobId());
             TaskInProgress tip = jip.findTaskInProgress(ts.getTaskId()
                     .getTaskID());
-
+                                                
             ts.setRunState(TaskStatus.State.FAILED);
+            System.out.println("Checking if recovery possible for task!");
             if (jip.handleFailure(tip)) {
+                System.out.println("Recovering now.");
                 recoverTask(jip);
             }
         }
       }
 
-
       // update GroomServerStatus held in the groomServers cache.
       GroomServerStatus groomStatus = ((ReportGroomStatusDirective) directive)
           .getStatus();
@@ -343,10 +345,14 @@ public class BSPMaster implements JobSubmissionProtocol, MasterProtocol,
       infoServer.start();
 
       if (conf.getBoolean("bsp.monitor.fd.enabled", false)) {
+        System.out.println("Fault Tolerance is enabled.");
         this.supervisor.set(FDProvider.createSupervisor(conf.getClass(
             "bsp.monitor.fd.supervisor.class", UDPSupervisor.class,
             Supervisor.class), conf));
       }
+      else{
+        System.out.println("Fault Tolerance is disabled.");
+      }
 
       while (!Thread.currentThread().isInterrupted()) {
         try {
diff --git a/core/src/main/java/org/apache/hama/bsp/BSPPeer.java b/core/src/main/java/org/apache/hama/bsp/BSPPeer.java
index 7c9cf4d..4e6a0b3 100644
--- a/core/src/main/java/org/apache/hama/bsp/BSPPeer.java
+++ b/core/src/main/java/org/apache/hama/bsp/BSPPeer.java
@@ -68,6 +68,21 @@ public interface BSPPeer<K1, V1, K2, V2, M extends Writable> extends Constants {
   public void sync() throws IOException, SyncException, InterruptedException;
 
   /**
+   * Called from a recovering peer. Fetches data from alive peer to reconstruct its state.
+   */
+  public void getPrevSuperstepData();
+  
+  /**
+   * Returns true if the peer is running a recovery task
+   */
+  public boolean isRecoveryTask();
+
+  /*
+   * Handles first sync for recovering task
+   */
+  public void doFirstSyncAfterRecovery() throws SyncException;
+
+  /**
    * @return the count of current super-step
    */
   public long getSuperstepCount();
diff --git a/core/src/main/java/org/apache/hama/bsp/BSPPeerImpl.java b/core/src/main/java/org/apache/hama/bsp/BSPPeerImpl.java
index 1242911..e7ca530 100644
--- a/core/src/main/java/org/apache/hama/bsp/BSPPeerImpl.java
+++ b/core/src/main/java/org/apache/hama/bsp/BSPPeerImpl.java
@@ -96,6 +96,9 @@ public final class BSPPeerImpl<K1, V1, K2, V2, M extends Writable> implements
 
   private long splitSize = 0L;
 
+  // Recovery
+  private boolean recoveryTask = false;
+
   /**
    * Protected default constructor for LocalBSPRunner.
    */
@@ -212,7 +215,9 @@ public final class BSPPeerImpl<K1, V1, K2, V2, M extends Writable> implements
             .onPeerInitialized(state);
 
         if (state == TaskStatus.State.RECOVERING) {
+          this.recoveryTask = true;
           if (newState == TaskStatus.State.RUNNING) {
+            LOG.info("[BSPPeerImpl.java] Changing taskState from RECOVERING to RUNNING");
             phase = TaskStatus.Phase.STARTING;
             stateString = "running";
             state = newState;
@@ -233,7 +238,8 @@ public final class BSPPeerImpl<K1, V1, K2, V2, M extends Writable> implements
       }
     }
 
-    doFirstSync(superstep);
+    if(this.recoveryTask == false)
+      doFirstSync(superstep);
 
     if (LOG.isDebugEnabled()) {
       LOG.info(new StringBuffer("BSP Peer successfully initialized for ")
@@ -281,6 +287,10 @@ public final class BSPPeerImpl<K1, V1, K2, V2, M extends Writable> implements
     return splitSize;
   }
 
+  public boolean isRecoveryTask() {
+    return recoveryTask;
+  }
+
   /**
    * @return the position in the input stream.
    */
@@ -311,6 +321,24 @@ public final class BSPPeerImpl<K1, V1, K2, V2, M extends Writable> implements
     syncClient.leaveBarrier(taskId.getJobID(), taskId, superstep);
   }
 
+  // At this point, the restarting peer has re-done the compute part
+  public void doFirstSyncAfterRecovery() throws SyncException {
+    //TODO: send out the messages from the current computation to the peers.
+    //Could reuse code from first part of sync(). At the receiving peer, handle
+    //duplicate messages (if any)
+
+    // TODO: this verifies if the peer crash happened between the enterBarrier() and
+    // leaveBarrier() calls. If yes, we have a problem because the alive peers
+    // have made a call to clearOutgoingMessages() which clears outgoing message
+    // bundles and their localQueues.
+    boolean check = syncClient.check(taskId, currentTaskStatus.getSuperstepCount());
+    if(!check)
+      enterBarrier();
+
+    leaveBarrier();
+    incrementCounter(PeerCounter.SUPERSTEP_SUM, 1L);
+  }
+
   @SuppressWarnings("unchecked")
   public final void initializeIO() throws Exception {
 
@@ -397,6 +425,7 @@ public final class BSPPeerImpl<K1, V1, K2, V2, M extends Writable> implements
     }
 
     long startBarrier = System.currentTimeMillis();
+
     enterBarrier();
 
     if (this.faultToleranceService != null) {
@@ -430,6 +459,15 @@ public final class BSPPeerImpl<K1, V1, K2, V2, M extends Writable> implements
 
   }
 
+  @Override
+  public final void getPrevSuperstepData() {
+      for(String peerName : allPeers) {
+          if(!peerName.equals(getPeerName())) {
+              messenger.getRecoveryData(peerName);
+          }   
+      }   
+  }
+  
   protected final void enterBarrier() throws SyncException {
     syncClient.enterBarrier(taskId.getJobID(), taskId,
         currentTaskStatus.getSuperstepCount());
diff --git a/core/src/main/java/org/apache/hama/bsp/BSPTask.java b/core/src/main/java/org/apache/hama/bsp/BSPTask.java
index f4fc14e..c1a9927 100644
--- a/core/src/main/java/org/apache/hama/bsp/BSPTask.java
+++ b/core/src/main/java/org/apache/hama/bsp/BSPTask.java
@@ -141,6 +141,7 @@ public final class BSPTask extends Task {
 
     startPingingGroom(job, umbilical);
     try {
+      LOG.info("Starting runBSP from BSPTask");
       runBSP(job, bspPeer, split, umbilical);
       done(umbilical);
     } finally {
@@ -167,8 +168,13 @@ public final class BSPTask extends Task {
     // The policy is to throw the first exception and log the remaining.
     Exception firstException = null;
     try {
+      LOG.info("[BSPTask.java] Starting with bsp.setup");
       bsp.setup(bspPeer);
+      
+      LOG.info("[BSPTask.java] Starting with bsp.bsp");
       bsp.bsp(bspPeer);
+
+      LOG.info("[BSPTask.java] Done with bsp.bsp");
     } catch (Exception e) {
       LOG.error("Error running bsp setup and bsp function.", e);
       firstException = e;
diff --git a/core/src/main/java/org/apache/hama/bsp/GroomServer.java b/core/src/main/java/org/apache/hama/bsp/GroomServer.java
index 8f21648..84add3b 100644
--- a/core/src/main/java/org/apache/hama/bsp/GroomServer.java
+++ b/core/src/main/java/org/apache/hama/bsp/GroomServer.java
@@ -973,6 +973,7 @@ public class GroomServer implements Runnable, GroomProtocol, BSPPeerProtocol,
 
     public void markAsRecoveryTask(long superstepNumber) {
       if (this.taskStatus.getRunState() != TaskStatus.State.FAILED) {
+        LOG.info("Starting recovery task at superstep " + superstepNumber);
         this.taskStatus.setRunState(TaskStatus.State.RECOVERING);
         this.taskStatus.setPhase(TaskStatus.Phase.RECOVERING);
         this.taskStatus.setStateString("recovering");
@@ -1012,7 +1013,10 @@ public class GroomServer implements Runnable, GroomProtocol, BSPPeerProtocol,
 
     public void launchTask() throws IOException {
       localizeTask(task);
-      taskStatus.setRunState(TaskStatus.State.RUNNING);
+      
+      if(taskStatus.getRunState() != TaskStatus.State.RECOVERING)
+        taskStatus.setRunState(TaskStatus.State.RUNNING);
+      
       this.runner = task.createRunner(GroomServer.this);
       this.runner.start();
       startTime = Calendar.getInstance().getTimeInMillis();
@@ -1213,6 +1217,7 @@ public class GroomServer implements Runnable, GroomProtocol, BSPPeerProtocol,
       if (LOG.isDebugEnabled())
         LOG.debug("BSPPeerChild starting");
 
+      LOG.info("BSPPeerChild starting");
       final HamaConfiguration defaultConf = new HamaConfiguration();
       // report address
       String host = args[0];
@@ -1240,18 +1245,21 @@ public class GroomServer implements Runnable, GroomProtocol, BSPPeerProtocol,
       long superstep = Long.parseLong(args[4]);
       TaskStatus.State state = TaskStatus.State.valueOf(args[5]);
       LOG.debug("Starting peer for step " + superstep + " state = " + state);
+      LOG.info("Starting peer for step " + superstep + " state = " + state);
 
       try {
         // use job-specified working directory
         FileSystem.get(job.getConfiguration()).setWorkingDirectory(
             job.getWorkingDirectory());
-
+              
+        LOG.info("instantiating BSPPeerImpl");
         // instantiate and init our peer
         @SuppressWarnings("rawtypes")
         final BSPPeerImpl<?, ?, ?, ?, ?> bspPeer = new BSPPeerImpl(job,
             defaultConf, taskid, umbilical, task.partition, task.splitClass,
             task.split, task.getCounters(), superstep, state);
 
+        LOG.info("instantiating BSPPeerImpl done, starting BSPTask now!");
         task.run(job, bspPeer, umbilical); // run the task
 
       } catch (FSError e) {
diff --git a/core/src/main/java/org/apache/hama/bsp/JobInProgress.java b/core/src/main/java/org/apache/hama/bsp/JobInProgress.java
index e49e3b7..a474ad3 100644
--- a/core/src/main/java/org/apache/hama/bsp/JobInProgress.java
+++ b/core/src/main/java/org/apache/hama/bsp/JobInProgress.java
@@ -367,9 +367,16 @@ public class JobInProgress {
     Task result = null;
     String[] selectedGrooms = taskAllocationStrategy.selectGrooms(
         groomStatuses, taskCountInGroomMap, resources, task);
+    System.out.println("Size of selectedGrooms set is " + selectedGrooms.length);
+    
+    // printing out the groom map.
+    for(String l : groomStatuses.keySet())
+      System.out.println("Groom:" + groomStatuses.get(l));
+
     GroomServerStatus groomStatus = taskAllocationStrategy
         .getGroomToAllocate(groomStatuses, selectedGrooms,
             taskCountInGroomMap, resources, task);
+    System.out.println("Groom server picked for task is " + groomStatus.getGroomName());
     if (groomStatus != null) {
       result = task.constructTask(groomStatus);
     } else if (LOG.isDebugEnabled()) {
diff --git a/core/src/main/java/org/apache/hama/bsp/LocalBSPRunner.java b/core/src/main/java/org/apache/hama/bsp/LocalBSPRunner.java
index 69fbaee..b3653cb 100644
--- a/core/src/main/java/org/apache/hama/bsp/LocalBSPRunner.java
+++ b/core/src/main/java/org/apache/hama/bsp/LocalBSPRunner.java
@@ -362,6 +362,10 @@ public class LocalBSPRunner implements JobSubmissionProtocol {
     public InetSocketAddress getListenerAddress() {
       return selfAddress;
     }
+
+    @Override
+    public void getRecoveryData (String peerName) {
+    }
   }
 
   public static class LocalUmbilical implements BSPPeerProtocol {
diff --git a/core/src/main/java/org/apache/hama/bsp/TaskInProgress.java b/core/src/main/java/org/apache/hama/bsp/TaskInProgress.java
index e1dca2c..ec88a3f 100644
--- a/core/src/main/java/org/apache/hama/bsp/TaskInProgress.java
+++ b/core/src/main/java/org/apache/hama/bsp/TaskInProgress.java
@@ -272,7 +272,8 @@ public class TaskInProgress {
       int attemptId = job.getNumRestarts() * NUM_ATTEMPTS_PER_RESTART
           + nextTaskId;
       taskid = new TaskAttemptID(id, attemptId);
-      ++nextTaskId;
+      // Let the recovery task come up with the exact same id
+      //++nextTaskId;
     } else {
       LOG.warn("Exceeded limit of " + (MAX_TASK_EXECS + maxTaskAttempts)
           + " attempts for the tip '" + getTIPId() + "'");
diff --git a/core/src/main/java/org/apache/hama/bsp/ft/AsyncRcvdMsgCheckpointImpl.java b/core/src/main/java/org/apache/hama/bsp/ft/AsyncRcvdMsgCheckpointImpl.java
index f03a62c..af5690a 100644
--- a/core/src/main/java/org/apache/hama/bsp/ft/AsyncRcvdMsgCheckpointImpl.java
+++ b/core/src/main/java/org/apache/hama/bsp/ft/AsyncRcvdMsgCheckpointImpl.java
@@ -136,9 +136,12 @@ public class AsyncRcvdMsgCheckpointImpl<M extends Writable> implements
       }
 
       long lowestSuperstepNumber = Long.MAX_VALUE;
+       
+      //String[] taskProgress = this.masterSyncClient.getChildKeySet(
+      //    this.masterSyncClient.constructKey(jobId, "checkpoint"), null);
 
       String[] taskProgress = this.masterSyncClient.getChildKeySet(
-          this.masterSyncClient.constructKey(jobId, "checkpoint"), null);
+          this.masterSyncClient.constructKey(jobId, "superstepRegisterAfterBarrier"), null);
 
       if (LOG.isDebugEnabled()) {
         StringBuffer list = new StringBuffer(25 * taskProgress.length);
@@ -150,7 +153,9 @@ public class AsyncRcvdMsgCheckpointImpl<M extends Writable> implements
         LOG.debug(list);
       }
 
-      if (taskProgress.length == this.tasks.length) {
+      // code used only when checkpointing is enabled
+      //if (taskProgress.length == this.tasks.length) {
+      if (false) {
         for (String taskProgres : taskProgress) {
           ArrayWritable progressInformation = new ArrayWritable(
               LongWritable.class);
@@ -181,7 +186,28 @@ public class AsyncRcvdMsgCheckpointImpl<M extends Writable> implements
             allTasksInProgress, taskCountInGroomMap, actionMap);
 
       } else {
-        restartJob(-1, groomStatuses, recoverySet, allTasksInProgress,
+        for (String taskProgres : taskProgress) {
+          ArrayWritable progressInformation = new ArrayWritable(LongWritable.class);  
+          boolean result = this.masterSyncClient.getInformation(
+            this.masterSyncClient.constructKey(jobId, "superstepRegisterAfterBarrier",
+                taskProgres), progressInformation);
+
+          if(!result) {
+            lowestSuperstepNumber = -1L;
+            break;
+          }
+
+          Writable[] progressArr = progressInformation.get();
+          LongWritable superstepProgress = (LongWritable) progressArr[0];
+        
+          if(superstepProgress != null) {
+            if(superstepProgress.get() < lowestSuperstepNumber) {
+              lowestSuperstepNumber = superstepProgress.get();
+            }
+          }
+        }
+
+        restartJob(lowestSuperstepNumber, groomStatuses, recoverySet, allTasksInProgress,
             taskCountInGroomMap, actionMap);
       }
 
@@ -222,7 +248,8 @@ public class AsyncRcvdMsgCheckpointImpl<M extends Writable> implements
         throws IOException {
       String path = conf.get("bsp.checkpoint.prefix_path", "/checkpoint/");
 
-      if (superstep >= 0) {
+      //if (superstep >= 0) {
+      if (false) {
         FileSystem fileSystem = FileSystem.get(conf);
         for (TaskInProgress allTask : allTasks) {
           String[] hosts = null;
@@ -257,20 +284,24 @@ public class AsyncRcvdMsgCheckpointImpl<M extends Writable> implements
             Task task = allTask.constructTask(serverStatus);
             populateAction(task, superstep, serverStatus, actionMap);
 
-          } else {
-            restartTask(allTask, superstep, groomStatuses, actionMap);
-          }
+          } //else {
+            //restartTask(allTask, superstep, groomStatuses, actionMap);
+          //}
         }
       } else {
         // Start the task from the beginning.
         for (TaskInProgress allTask : allTasks) {
           if (recoveryMap.containsKey(allTask.getTaskId())) {
-            this.allocationStrategy.getGroomToAllocate(groomStatuses,
+
+            GroomServerStatus serverStatus = this.allocationStrategy.getGroomToAllocate(groomStatuses,
                 this.allocationStrategy.selectGrooms(groomStatuses,
                     taskCountInGroomMap, new BSPResource[0], allTask),
                 taskCountInGroomMap, new BSPResource[0], allTask);
+            Task task = allTask.constructTask(serverStatus);
+            populateAction(task, superstep, serverStatus, actionMap);
+
           } else {
-            restartTask(allTask, superstep, groomStatuses, actionMap);
+            //restartTask(allTask, superstep, groomStatuses, actionMap);
           }
         }
       }
@@ -364,6 +395,27 @@ public class AsyncRcvdMsgCheckpointImpl<M extends Writable> implements
       return ckptPath;
     }
 
+    // Ask the alive bspPeers for information so as to construct own state after failure recovery
+    @Override
+    public TaskStatus.State onPeerInitialized(TaskStatus.State state)
+            throws Exception{
+
+        if(this.superstep >= 0 && state.equals(TaskStatus.State.RECOVERING)){
+            String thisPeerName = peer.getPeerName();
+            LOG.info("bspPeer " + thisPeerName + " started pinging alive peers for data");
+            
+            // TODO: Test this.
+            // We also need to fetch the outgoingMessageBundle for 'this'
+            // superstep from alive peer so that the recovering peer can do the
+            // next superstep. Combine that with getPrevSuperstepData() ??
+            //peer.getPrevSuperstepData();
+            LOG.info("Data recovery complete for peer " + thisPeerName);
+        }
+        this.messenger.registerListener(this);
+        return TaskStatus.State.RUNNING;
+    }
+    
+    /*
     @Override
     public TaskStatus.State onPeerInitialized(TaskStatus.State state)
         throws Exception {
@@ -412,6 +464,7 @@ public class AsyncRcvdMsgCheckpointImpl<M extends Writable> implements
       return TaskStatus.State.RUNNING;
 
     }
+    */
 
     public final boolean isReadyToCheckpoint() {
 
@@ -447,6 +500,17 @@ public class AsyncRcvdMsgCheckpointImpl<M extends Writable> implements
     public void afterBarrier() throws Exception {
 
       synchronized (this) {
+        ArrayWritable writableArray = new ArrayWritable(LongWritable.class);
+        Writable[] writeArr = new Writable[1];
+        writeArr[0] = new LongWritable(peer.getSuperstepCount());
+        writableArray.set(writeArr);
+
+        this.syncClient.storeInformation(this.syncClient.constructKey(
+            this.job.getJobID(), "superstepRegisterAfterBarrier",
+            String.valueOf(peer.getPeerIndex())), writableArray, true, null);
+      }
+
+      synchronized (this) {
         if (checkpointState) {
 
           if (checkpointStream != null) {
diff --git a/core/src/main/java/org/apache/hama/bsp/message/AbstractOutgoingMessageManager.java b/core/src/main/java/org/apache/hama/bsp/message/AbstractOutgoingMessageManager.java
index f160015..9e0fbae 100644
--- a/core/src/main/java/org/apache/hama/bsp/message/AbstractOutgoingMessageManager.java
+++ b/core/src/main/java/org/apache/hama/bsp/message/AbstractOutgoingMessageManager.java
@@ -32,6 +32,10 @@ public abstract class AbstractOutgoingMessageManager<M extends Writable>
   
   protected final HashMap<String, InetSocketAddress> peerSocketCache = new HashMap<String, InetSocketAddress>();
   protected HashMap<InetSocketAddress, BSPMessageBundle<M>> outgoingBundles =  new HashMap<InetSocketAddress, BSPMessageBundle<M>>();
+  // outgoing bundle from the previous superstep 
+  protected HashMap<InetSocketAddress, BSPMessageBundle<M>> prevOutgoingBundles = new HashMap<InetSocketAddress, BSPMessageBundle<M>>();
+  // temporary reference for switching
+  protected HashMap<InetSocketAddress, BSPMessageBundle<M>> temp;
 
   protected InetSocketAddress getSocketAddress(String peerName) {
     InetSocketAddress targetPeerAddress = null;
diff --git a/core/src/main/java/org/apache/hama/bsp/message/HamaAsyncMessageManagerImpl.java b/core/src/main/java/org/apache/hama/bsp/message/HamaAsyncMessageManagerImpl.java
index 2b29ade..7505bc7 100644
--- a/core/src/main/java/org/apache/hama/bsp/message/HamaAsyncMessageManagerImpl.java
+++ b/core/src/main/java/org/apache/hama/bsp/message/HamaAsyncMessageManagerImpl.java
@@ -184,6 +184,14 @@ public final class HamaAsyncMessageManagerImpl<M extends Writable> extends
   }
 
   @Override
+  public void fetch(String requestingPeerName) throws IOException{
+  }                                             
+
+  @Override
+  public void getRecoveryData (String peerName) {
+  }
+
+  @Override
   public final long getProtocolVersion(String arg0, long arg1)
       throws IOException {
     return versionID;
diff --git a/core/src/main/java/org/apache/hama/bsp/message/HamaMessageManager.java b/core/src/main/java/org/apache/hama/bsp/message/HamaMessageManager.java
index 6e44d3a..4fda975 100644
--- a/core/src/main/java/org/apache/hama/bsp/message/HamaMessageManager.java
+++ b/core/src/main/java/org/apache/hama/bsp/message/HamaMessageManager.java
@@ -48,4 +48,5 @@ public interface HamaMessageManager<M extends Writable> extends
   
   public void put(byte[] compressedBundle) throws IOException;
 
+  public void fetch(String requestingPeerName) throws IOException;
 }
diff --git a/core/src/main/java/org/apache/hama/bsp/message/HamaMessageManagerImpl.java b/core/src/main/java/org/apache/hama/bsp/message/HamaMessageManagerImpl.java
index cdb1dc1..62b2dca 100644
--- a/core/src/main/java/org/apache/hama/bsp/message/HamaMessageManagerImpl.java
+++ b/core/src/main/java/org/apache/hama/bsp/message/HamaMessageManagerImpl.java
@@ -40,6 +40,7 @@ import org.apache.hama.ipc.HamaRPCProtocolVersion;
 import org.apache.hama.ipc.RPC;
 import org.apache.hama.ipc.RPC.Server;
 import org.apache.hama.util.LRUCache;
+import org.apache.hama.util.BSPNetUtils;
 
 /**
  * Implementation of the {@link HamaMessageManager}.
@@ -116,6 +117,26 @@ public final class HamaMessageManagerImpl<M extends Writable> extends
   }
 
   @Override
+  public void getRecoveryData(String peerName) {
+    InetSocketAddress targetPeerAddress = BSPNetUtils.getAddress(peerName);
+    
+    try {
+      HamaMessageManager<M> bspPeerConnection = this.getBSPPeerConnection(targetPeerAddress);
+      if(bspPeerConnection == null) {
+        throw new IllegalArgumentException("Can not find " + targetPeerAddress.toString()
+          + " to transfer messages to!");
+      }
+      else{
+        // make an RPC to alive peer with the requesting peer as argument 
+        bspPeerConnection.fetch(peer.getPeerName());
+      }
+    } catch (Exception e) {
+      LOG.info("Could not recovery data from peer " + peerName);
+      LOG.info(e.toString());
+    }
+  }
+
+  @Override
   public final void transfer(InetSocketAddress addr, BSPMessageBundle<M> bundle)
       throws IOException {
     HamaMessageManager<M> bspPeerConnection = this.getBSPPeerConnection(addr);
@@ -183,6 +204,17 @@ public final class HamaMessageManagerImpl<M extends Writable> extends
   }
 
   @Override
+  public final void fetch(String requestingPeerName)
+          throws IOException {
+      LOG.info("bspPeer " + peer.getPeerName() + " received request for previous superstep data from bspPeer " + requestingPeerName);
+      InetSocketAddress requestingPeerAddress = BSPNetUtils.getAddress(requestingPeerName);
+      BSPMessageBundle<M> bundle = outgoingMessageManager.getBundleFromPrevSuperstep(requestingPeerAddress);
+      transfer(requestingPeerAddress, bundle);
+      
+      // TODO: We also need to transfer some portion of the localQueue
+  }
+
+  @Override
   public final long getProtocolVersion(String arg0, long arg1)
       throws IOException {
     return versionID;
diff --git a/core/src/main/java/org/apache/hama/bsp/message/MessageManager.java b/core/src/main/java/org/apache/hama/bsp/message/MessageManager.java
index af4680a..2ebc405 100644
--- a/core/src/main/java/org/apache/hama/bsp/message/MessageManager.java
+++ b/core/src/main/java/org/apache/hama/bsp/message/MessageManager.java
@@ -73,6 +73,11 @@ public interface MessageManager<M extends Writable> {
   public void send(String peerName, M msg) throws IOException;
 
   /**
+   * Asks alive bspPeers to send data for restoration after failure.
+   */
+  public void getRecoveryData (String peerName);
+  
+  /**
    * Returns an bundle of messages grouped by peer.
    * 
    */
diff --git a/core/src/main/java/org/apache/hama/bsp/message/OutgoingMessageManager.java b/core/src/main/java/org/apache/hama/bsp/message/OutgoingMessageManager.java
index ba4ef91..008106a 100644
--- a/core/src/main/java/org/apache/hama/bsp/message/OutgoingMessageManager.java
+++ b/core/src/main/java/org/apache/hama/bsp/message/OutgoingMessageManager.java
@@ -35,4 +35,5 @@ public interface OutgoingMessageManager<M extends Writable> {
 
   public Iterator<Entry<InetSocketAddress, BSPMessageBundle<M>>> getBundleIterator();
 
+  public BSPMessageBundle<M> getBundleFromPrevSuperstep(InetSocketAddress peerAddress);
 }
diff --git a/core/src/main/java/org/apache/hama/bsp/message/OutgoingPOJOMessageBundle.java b/core/src/main/java/org/apache/hama/bsp/message/OutgoingPOJOMessageBundle.java
index 9d350e7..507022b 100644
--- a/core/src/main/java/org/apache/hama/bsp/message/OutgoingPOJOMessageBundle.java
+++ b/core/src/main/java/org/apache/hama/bsp/message/OutgoingPOJOMessageBundle.java
@@ -64,6 +64,11 @@ public class OutgoingPOJOMessageBundle<M extends Writable> extends
   }
 
   @Override
+  public BSPMessageBundle<M> getBundleFromPrevSuperstep(InetSocketAddress peerAddress) {
+      return prevOutgoingBundles.get(peerAddress); 
+  }
+  
+  @Override
   public void clear() {
     outgoingBundles.clear();
   }
diff --git a/core/src/main/java/org/apache/hama/bsp/sync/PeerSyncClient.java b/core/src/main/java/org/apache/hama/bsp/sync/PeerSyncClient.java
index 1ceaa25..8791892 100644
--- a/core/src/main/java/org/apache/hama/bsp/sync/PeerSyncClient.java
+++ b/core/src/main/java/org/apache/hama/bsp/sync/PeerSyncClient.java
@@ -62,6 +62,12 @@ public interface PeerSyncClient extends SyncClient {
       throws SyncException;
 
   /**
+   * Checks if a particular znode exists on zookeeper
+   */
+  public boolean check(TaskAttemptID taskId, long superstep)
+      throws SyncException;
+
+  /**
    * Registers a specific task with a its host and port to the sync daemon.
    * 
    * @param jobId the jobs ID
diff --git a/core/src/main/java/org/apache/hama/bsp/sync/ZooKeeperSyncClientImpl.java b/core/src/main/java/org/apache/hama/bsp/sync/ZooKeeperSyncClientImpl.java
index f147be2..00e2ea9 100644
--- a/core/src/main/java/org/apache/hama/bsp/sync/ZooKeeperSyncClientImpl.java
+++ b/core/src/main/java/org/apache/hama/bsp/sync/ZooKeeperSyncClientImpl.java
@@ -86,10 +86,26 @@ public class ZooKeeperSyncClientImpl extends ZKSyncClient implements
     numBSPTasks = conf.getInt("bsp.peers.num", 1);
   }
 
+  public boolean check(TaskAttemptID taskId, long superstep) 
+    throws SyncException {
+
+    try {
+      synchronized (zk) {
+        Stat s = zk.exists(getNodeName(taskId, superstep), false);
+        if(s == null) 
+          return false;
+        return true;
+      }
+    } catch (Exception e) {
+        throw new SyncException(e.toString());
+    }
+  }
+
   @Override
   public void enterBarrier(BSPJobID jobId, TaskAttemptID taskId, long superstep)
       throws SyncException {
     LOG.debug("[" + getPeerName() + "] enter the enterbarrier: " + superstep);
+    System.out.println(getPeerName() + " enter the enterbarrier: " + superstep);
 
     try {
       synchronized (zk) {
diff --git a/core/src/main/java/org/apache/hama/bsp/taskallocation/BestEffortDataLocalTaskAllocator.java b/core/src/main/java/org/apache/hama/bsp/taskallocation/BestEffortDataLocalTaskAllocator.java
index 87d0894..2cc6c39 100644
--- a/core/src/main/java/org/apache/hama/bsp/taskallocation/BestEffortDataLocalTaskAllocator.java
+++ b/core/src/main/java/org/apache/hama/bsp/taskallocation/BestEffortDataLocalTaskAllocator.java
@@ -79,9 +79,50 @@ public class BestEffortDataLocalTaskAllocator implements TaskAllocationStrategy
       Map<GroomServerStatus, Integer> tasksInGroomMap,
       String[] possibleLocations) {
 
+    // Round-robin allocation of tasks to Groom servers.
+                                
+    String groomServerWithMinTasks = null;
+    Integer minTasks = Integer.MAX_VALUE;
+
+    // find the groom server with the minimum number of tasks
+    for (String location : possibleLocations) {
+      GroomServerStatus groom = grooms.get(location);
+      if (groom == null) {
+        System.out.println("Could not find groom for location " + location);
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Could not find groom for location " + location);
+        }
+        continue;
+      }
+      
+      Integer tasksInGroom = tasksInGroomMap.get(groom);
+      tasksInGroom = (tasksInGroom == null) ? 0 : tasksInGroom;
+
+      if(tasksInGroom <= minTasks) {
+        groomServerWithMinTasks = location;
+        minTasks = tasksInGroom;
+      }
+    }
+
+    // No groom server seems to be alive OR the number of tasks on each groom
+    // server has reached the maximum. (Assumes that each groomserver has equal
+    // task-holding capacity)
+    if(groomServerWithMinTasks == null || minTasks == grooms.get(groomServerWithMinTasks).getMaxTasks()) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Returning null");
+      }
+      System.out.println("No groom server found for task.");
+      return null;
+    }
+
+    System.out.println("Task allocated to groom server " + groomServerWithMinTasks + ". Existing tasks on this groom server = " + minTasks);
+    return groomServerWithMinTasks;
+
+    /*
     for (String location : possibleLocations) {
       GroomServerStatus groom = grooms.get(location);
       if (groom == null) {
+        System.out.println("Could not find groom for location " + location);
         if (LOG.isDebugEnabled()) {
           LOG.debug("Could not find groom for location " + location);
         }
@@ -94,8 +135,10 @@ public class BestEffortDataLocalTaskAllocator implements TaskAllocationStrategy
             + groom.getMaxTasks() + " location = " + location
             + " groomhostname = " + groom.getGroomHostName());
       }
+      System.out.println("getMaxTasks() for groom " + groom.getGroomName() + " returns " + groom.getMaxTasks() + ". Tasks already in groom " + taskInGroom);
       if (taskInGroom < groom.getMaxTasks()
           && location.equals(groom.getGroomHostName())) {
+        System.out.println("Allocating on groom " + groom.getGroomName());
         return groom.getGroomHostName();
       }
     }
@@ -103,6 +146,7 @@ public class BestEffortDataLocalTaskAllocator implements TaskAllocationStrategy
       LOG.debug("Returning null");
     }
     return null;
+    */
   }
 
   @Override
diff --git a/core/target/generated-sources/annotations/org/apache/hama/package-info.java b/core/target/generated-sources/annotations/org/apache/hama/package-info.java
index 81750e5..9d332b2 100644
--- a/core/target/generated-sources/annotations/org/apache/hama/package-info.java
+++ b/core/target/generated-sources/annotations/org/apache/hama/package-info.java
@@ -1,7 +1,7 @@
 /*
  * Generated by src/saveVersion.sh
  */
-@HamaVersionAnnotation(version="0.7.0-SNAPSHOT", revision="eb34409da5a58fead35a8a0439b23cb1e2a3b55b", branch="master",
-                         user="root", date="Fri Mar 20 19:57:43 CDT 2015", url="git://wirelessprvnat-172-17-35-168.near.illinois.edu/usr/local/hama-trunk/core",
-                         srcChecksum="Not Available")
+@HamaVersionAnnotation(version="0.7.0-SNAPSHOT", revision="Unknown", branch="Unknown",
+                         user="vagrant", date="Sat Apr 11 11:20:25 UTC 2015", url="file:///vagrant/hama/core",
+                         srcChecksum="43fea4085479ddd72707bc8093ac9e9b")
 package org.apache.hama;
diff --git a/graph/src/main/java/org/apache/hama/graph/GraphJobRunner.java b/graph/src/main/java/org/apache/hama/graph/GraphJobRunner.java
index 6f9db29..9cc1412 100644
--- a/graph/src/main/java/org/apache/hama/graph/GraphJobRunner.java
+++ b/graph/src/main/java/org/apache/hama/graph/GraphJobRunner.java
@@ -101,25 +101,48 @@ public final class GraphJobRunner<V extends WritableComparable, E extends Writab
   // -1 is deactivated
   private int maxIteration = -1;
   private long iteration;
+  private boolean recoveryTask = false;
 
   private AggregationRunner<V, E, M> aggregationRunner;
   private VertexOutputWriter<Writable, Writable, V, E, M> vertexOutputWriter;
 
   private BSPPeer<Writable, Writable, Writable, Writable, GraphJobMessage> peer;
 
+  private void redoSuperstep() {
+    
+    // update the value of local variables
+    iteration = peer.getSuperstepCount(); 
+    //TODO : fix this! We are not calling countGlobalVertexCount() for
+    //recovering peer. We need to get from alive peer. Maybe similar to how to
+    //we superstep data?
+    numberVertices = 50;
+
+    // TODO : redo the compute part here. Don't call peer.sync()! It would be
+    // handled in peer.doFirstSyncAfterRecovery() which is called just after
+    // this function.
+  }
+
   @Override
   public final void setup(
       BSPPeer<Writable, Writable, Writable, Writable, GraphJobMessage> peer)
       throws IOException, SyncException, InterruptedException {
-
+ 
+    LOG.info("[GraphJobRunner] Entering setup.");  
+    recoveryTask = peer.isRecoveryTask();
+      
     setupFields(peer);
 
     loadVertices(peer);
 
-    countGlobalVertexCount(peer);
+    if(recoveryTask == false)
+      countGlobalVertexCount(peer);
 
     doInitialSuperstep(peer);
 
+    if(recoveryTask) {
+      redoSuperstep();
+      peer.doFirstSyncAfterRecovery();
+    }
   }
 
   @Override
@@ -138,7 +161,12 @@ public final class GraphJobRunner<V extends WritableComparable, E extends Writab
       // note that the messages must be parsed here
       GraphJobMessage firstVertexMessage = parseMessages(peer);
       // master/slaves needs to update
-      doAggregationUpdates(peer);
+      
+      // TODO: For recovery task, sendAggregatorValues() is not called in
+      // doInitialSuperstep(). This seems to mess up with the aggregator
+      // functionality.  
+      //doAggregationUpdates(peer);
+      
       // check if updated changed by our aggregators
       if (!updated) {
         break;
@@ -288,7 +316,10 @@ public final class GraphJobRunner<V extends WritableComparable, E extends Writab
     }
 
     vertices.finishSuperstep();
-    getAggregationRunner().sendAggregatorValues(peer, 1, this.changedVertexCnt);
+
+    if(recoveryTask == false)
+      getAggregationRunner().sendAggregatorValues(peer, 1, this.changedVertexCnt);
+
     iteration++;
   }
 
diff --git a/graph/src/main/java/org/apache/hama/graph/OutgoingVertexMessageManager.java b/graph/src/main/java/org/apache/hama/graph/OutgoingVertexMessageManager.java
index 3694e80..9a420d1 100644
--- a/graph/src/main/java/org/apache/hama/graph/OutgoingVertexMessageManager.java
+++ b/graph/src/main/java/org/apache/hama/graph/OutgoingVertexMessageManager.java
@@ -85,8 +85,24 @@ public class OutgoingVertexMessageManager<M extends Writable> extends
   }
 
   @Override
+  public BSPMessageBundle<GraphJobMessage> getBundleFromPrevSuperstep(InetSocketAddress peerAddress) {
+    // TODO: Need to return back data from storage HashMap as well. Could do it
+    // similar to getBundleIterator() function below.
+    return prevOutgoingBundles.get(peerAddress);
+  }
+
+  @Override
   public void clear() {
-    outgoingBundles.clear();
+
+    // switch
+    prevOutgoingBundles.clear();
+    temp = prevOutgoingBundles;
+    prevOutgoingBundles = outgoingBundles;
+    outgoingBundles = temp;
+    //outgoingBundles.clear();
+    
+    // TODO: switch for storage
+
     storage.clear();
   }
 
